#---------------Esse script irá ler os arquivos do ancombc, excluir colunas indesejadas e aplicar filtro (p-value e q-value) em conjunto com o filtro de log-fold-change 

import pandas as pd

#Transformar ancombc em csv pelo qiime2, ele irá gerar lfc_slice.csv;p_val_slice.csv;q_val_slice.csv
#bash linux
#qiime tools export --input-path ancombc-fdr-adjusted.qza --output-path ancombc_csv

# Substitua 'planilha1.xlsx', 'planilha2.xlsx', 'planilha3.xlsx' pelos caminhos das suas planilhas
# e 'colunaA', 'colunaB', etc., pelos nomes reais das colunas que você deseja extrair.

# Leitura das planilhas em tsv usando sep='\t'
df1 = pd.read_csv('../lfc_slice.csv', sep=',')
df2 = pd.read_csv('../p_val_slice.csv', sep=',')
df3 = pd.read_csv('../q_val_slice.csv', sep=',')

# Adicionando uma nova coluna 'Dataset' na primeira posição
#Se tiver várias variáveis no mesmo dataset, colocar além do nome do dataset qual a variável. ex: PRJNA.._crypto
df1.insert(0, 'Dataset', 'PRJEB26004')
df2.insert(0, 'Dataset', 'PRJEB26004')
df3.insert(0, 'Dataset', 'PRJEB26004')

#Excluindo as colunas indesejadas

# Excluir múltiplas colunas
# Se precisar excluir outras colunas, adicionar:
# df1 = df1.drop(['(Intercept)', 'Group_Helm_Pro'], axis=1)

df1 = df1.drop(['(Intercept)'], axis=1)
df2 = df2.drop(['(Intercept)'], axis=1)
df3 = df3.drop(['(Intercept)'], axis=1)

#Renomear as colunas para depois que der merge não virar _x e _y

# Renomear colunas usando .rename()
df1 = df1.rename(columns={'diagnosisSZ': 'diagnosisSZ_lfc'})
df2 = df2.rename(columns={'diagnosisSZ': 'diagnosisSZ_pval'})
df3 = df3.rename(columns={'diagnosisSZ': 'diagnosisSZ_qval'})

# Extração das colunas da Planilha 1
# Substituir pelos nomes das colunas 
df1_extracted = df1[['id', 'diagnosisSZ_lfc']]

#Filtrando pelo log-fold-change em ordem decrescente
df1_filtered = df1.sort_values(by='diagnosisSZ_lfc', ascending=False)

# Extração da coluna da Planilha 2 e aplicação do filtro p-value < 0.05
df2_extracted_filtered = df2[['id', 'diagnosisSZ_pval']][df2['diagnosisSZ_pval'] < 0.05]

# Extração da coluna da Planilha 3 e aplicação do filtro q-value < 0.2
df3_extracted_filtered = df3[['id', 'diagnosisSZ_qval']][df3['diagnosisSZ_qval'] < 0.2]

# Combinação dos dados extraídos e filtrados em um novo DataFrame

# Combinação com base em uma coluna 'ID' comum 
result_pval = pd.merge(df1_filtered, df2_extracted_filtered, on='id')
result_qval = pd.merge(df1_filtered, df3_extracted_filtered, on='id')

# Exportar o DataFrame resultante para um novo arquivo Excel ou CSV, se necessário
df1_filtered.to_csv('../result_lfc.tsv', sep='\t', index=False)
result_pval.to_csv('../result_pval.tsv', sep='\t', index=False)
result_qval.to_csv('../result_qval.tsv', sep='\t', index=False)

#---------------Essa parte irá ler os arquivos do ancombc já filtrados e colocar em ordem decrescente (para ficar na ordem os taxons aumentados (com log fold change positivo) e os taxons diminuídos (que tem log fold change negativo), e criar as colunas
#com os datasets dos taxons aumentados e os taxons correspondentes

# Lendo o DataFrame de lfc a partir do arquivo CSV
df_lfc = pd.read_csv('../result_lfc.tsv', sep='\t')
df_pval = pd.read_csv('../result_pval.tsv', sep='\t')
df_qval = pd.read_csv('../result_qval.tsv', sep='\t')

# Filtrando os valores positivos de lfc (sempre pela coluna de log-fold-change) e renomear nomes das colunas
df_lfc_enriched = df_lfc[df_lfc['diagnosisSZ_lfc'] > 0].copy()
df_lfc_enriched.rename(columns={'Dataset': 'Dataset_of_enriched_taxa'}, inplace=True)
df_lfc_enriched.rename(columns={'id': 'Enriched_taxa'}, inplace=True)

# Filtrando os valores positivos de p-val (sempre pela coluna de log-fold-change) e renomear nomes das colunas
df_pval_enriched = df_pval[df_pval['diagnosisSZ_lfc'] > 0].copy() 
df_pval_enriched.rename(columns={'Dataset': 'Dataset_of_enriched_taxa'}, inplace=True) 
df_pval_enriched.rename(columns={'id': 'Enriched_taxa'}, inplace=True)

# Filtrando os valores positivos de q-val (sempre pela coluna de log-fold-change) e renomear nomes das colunas
df_qval_enriched = df_qval[df_qval['diagnosisSZ_lfc'] > 0].copy()
df_qval_enriched.rename(columns={'Dataset': 'Dataset_of_enriched_taxa'}, inplace=True)
df_qval_enriched.rename(columns={'id': 'Enriched_taxa'}, inplace=True)

# Filtrando os valores negativos de lfc (sempre pela coluna de log-fold-change) e renomear nomes das colunas
df_lfc_depleted = df_lfc[df_lfc['diagnosisSZ_lfc'] < 0].copy()
df_lfc_depleted.rename(columns={'Dataset': 'Dataset_of_depleted_taxa'}, inplace=True)
df_lfc_depleted.rename(columns={'id': 'Depleted_taxa'}, inplace=True)

# Filtrando os valores negativos de p-val (sempre pela coluna de log-fold-change) e renomear nomes das colunas
df_pval_depleted = df_pval[df_pval['diagnosisSZ_lfc'] < 0].copy()
df_pval_depleted.rename(columns={'Dataset': 'Dataset_of_depleted_taxa'}, inplace=True)
df_pval_depleted.rename(columns={'id': 'Depleted_taxa'}, inplace=True)

# Filtrando os valores negativos de q-val (sempre pela coluna de log-fold-change) e renomear nomes das colunas
df_qval_depleted = df_qval[df_qval['diagnosisSZ_lfc'] < 0].copy()
df_qval_depleted.rename(columns={'Dataset': 'Dataset_of_depleted_taxa'}, inplace=True)
df_qval_depleted.rename(columns={'id': 'Depleted_taxa'}, inplace=True)

#Salvar os arquivos
df_lfc_enriched.to_csv('../df_lfc_enriched.tsv', sep='\t', index=False)
df_lfc_depleted.to_csv('../df_lfc_depleted.tsv', sep='\t', index=False)
df_pval_enriched.to_csv('../df_pval_enriched.tsv', sep='\t', index=False)
df_pval_depleted.to_csv('../df_pval_depleted.tsv', sep='\t', index=False)
df_qval_enriched.to_csv('../df_qval_enriched.tsv', sep='\t', index=False)
df_qval_depleted.to_csv('../df_qval_depleted.tsv', sep='\t', index=False)

#---------------Essa parte irá juntar os diferentes arquivos com os datasets e taxons aumentados/diminuídos para criar uma tabelona com todos os taxons aumentados e diminuídos

# Caminhos para os arquivos CSV
caminho_enriched_1 = '../df_lfc_enriched.tsv'
caminho_depleted_1 = '../df_lfc_depleted.tsv'

caminho_enriched_2 = '../df_lfc_enriched.tsv'
caminho_depleted_2 = '../df_lfc_depleted.tsv'

caminho_enriched_3 = '../df_lfc_enriched.tsv'
caminho_depleted_3 = '../df_lfc_depleted.tsv'


# Ler os arquivos CSV para DataFrames
df_enriched_1 = pd.read_csv(caminho_enriched_1, sep='\t')
df_enriched_2 = pd.read_csv(caminho_enriched_2, sep='\t')
df_enriched_3 = pd.read_csv(caminho_enriched_3, sep='\t')

df_depleted_1 = pd.read_csv(caminho_depleted_1, sep='\t')
df_depleted_2 = pd.read_csv(caminho_depleted_2, sep='\t')
df_depleted_3 = pd.read_csv(caminho_depleted_3, sep='\t')

'''
# Selecionar e mesclar os DataFrames enriquecidos
#df_enriched_merged = pd.concat([df_enriched_1[['Dataset_of_enriched_taxa', 'Enriched_taxa']], 
                         df_enriched_3[['Dataset_of_enriched_taxa', 'Enriched_taxa']],
                                df_enriched_4[['Dataset_of_enriched_taxa', 'Enriched_taxa']],
                                df_enriched_5[['Dataset_of_enriched_taxa', 'Enriched_taxa']],
                                df_enriched_6[['Dataset_of_enriched_taxa', 'Enriched_taxa']],
                                df_enriched_7[['Dataset_of_enriched_taxa', 'Enriched_taxa']],
                                df_enriched_8[['Dataset_of_enriched_taxa', 'Enriched_taxa']],
                                df_enriched_9[['Dataset_of_enriched_taxa', 'Enriched_taxa']],
                               ]) '''

#Substituindo o comando acima com loop:

# Colocar todos os DataFrames enriched em uma lista
dfs_enriched = [df_enriched_1, df_enriched_2, df_enriched_3]

# Usar um loop para selecionar as colunas desejadas de cada DataFrame
dfs_enriched_columns_selected = [df[['Dataset_of_enriched_taxa', 'Enriched_taxa']] for df in dfs_enriched]

# Concatenar todos os DataFrames selecionados
df_enriched_merged = pd.concat(dfs_enriched_columns_selected)

# Colocar todos os DataFrames depleted em uma lista
dfs_depleted = [df_depleted_1, df_depleted_2, df_depleted_3]

# Usar um loop para selecionar as colunas desejadas de cada DataFrame
dfs_depleted_columns_selected = [df[['Dataset_of_depleted_taxa', 'Depleted_taxa']] for df in dfs_depleted]

# Concatenar todos os DataFrames selecionados
df_depleted_merged = pd.concat(dfs_depleted_columns_selected)

# Juntar os DataFrames aumentados e diminuídos, lado a lado
# Como eles podem não ter o mesmo número de linhas, ajustaremos o índice para permitir a concatenação.
df_final = pd.concat([df_enriched_merged.reset_index(drop=True), df_depleted_merged.reset_index(drop=True)], axis=1)

# Salvando o DataFrame final como TSV
caminho_final_tsv = '../final_merged_data.tsv'
df_final.to_csv(caminho_final_tsv, sep='\t', index=False)

#---------------Essa parte irá calcular os taxons consistentes, ou seja: vai ler as colunas de taxons aumentados e diminuídos, cruzar ambas individualmente e uma contra a outra e retornar na coluna "Presenca" se o taxon está aumentado e/ou diminuído

df = pd.read_csv('../final_merged_data.tsv', sep='\t')

# Dicionário para armazenar a contagem, os datasets associados e a origem para cada taxa
taxa_info = {}

# Processar colunas de taxons aumentados
for idx, row in df.iterrows():
    taxa = row['Enriched_taxa']
    dataset = row['Dataset_of_enriched_taxa']
    origem = 'Enriched'
    if pd.notnull(taxa):
        chave = (taxa, origem)  # Chave única que considera o taxon e sua origem
        if chave not in taxa_info:
            taxa_info[chave] = {'Contagem': 1, 'Datasets': set([dataset])}
        else:
            taxa_info[chave]['Contagem'] += 1
            taxa_info[chave]['Datasets'].add(dataset)

# Processar colunas de taxons diminuídos
for idx, row in df.iterrows():
    taxa = row['Depleted_taxa']
    dataset = row['Dataset_of_depleted_taxa']
    origem = 'Depleted'
    if pd.notnull(taxa):
        chave = (taxa, origem)  # Chave única considera a taxa e sua origem
        if chave not in taxa_info:
            taxa_info[chave] = {'Contagem': 1, 'Datasets': set([dataset])}
        else:
            taxa_info[chave]['Contagem'] += 1
            taxa_info[chave]['Datasets'].add(dataset)

# Converter o dicionário em um DataFrame para melhor visualização
resultados = []
for (taxa, origem), info in taxa_info.items():
    for dataset in info['Datasets']:  # Loop para adicionar múltiplos datasets se necessário
        resultados.append([taxa, info['Contagem'], dataset, origem])

df_resultado = pd.DataFrame(resultados, columns=['Taxa', 'Contagem', 'Dataset', 'Origem'])

# Filtrar para incluir apenas taxons com contagem >= 2
df_filtrado = df_resultado.query("Contagem >= 2")

# Inicialização do dicionário para mapear a presença de cada taxa no df_filtrado
presenca_taxa_filtrado = {}

# Processamento do df_filtrado para determinar a presença de cada taxon
for idx, row in df_filtrado.iterrows():
    taxa = row['Taxa']
    origem = row['Origem']
    # Atualiza o dicionário com a origem para cada taxon no df_filtrado
    if taxa not in presenca_taxa_filtrado:
        presenca_taxa_filtrado[taxa] = {origem}
    else:
        presenca_taxa_filtrado[taxa].add(origem)

# Função para determinar a categoria de presença para cada taxon no df_filtrado
def determinar_categoria_filtrado(taxa):
    origens = presenca_taxa_filtrado[taxa]
    if 'Enriched' in origens and 'Depleted' in origens:
        return 'Both'
    elif 'Enriched' in origens:
        return 'Only enriched'
    elif 'Depleted' in origens:  # Garante que a lógica cobre todas as possibilidades
        return 'Only depleted'
    else:
        return 'Undetermined'  # Para segurança, caso algum taxon não caia nas categorias esperadas

# Aplicação da função ao df_filtrado para criar a nova coluna 'Presença'
df_filtrado['Presenca'] = df_filtrado['Taxa'].apply(determinar_categoria_filtrado)

# Exibição do df_filtrado atualizado
print(df_filtrado)

# Definindo o caminho completo para o arquivo TSV
caminho_do_arquivo = '../resultado-ancombc-adultos-p-valor.tsv'

# Salvando o DataFrame df_filtrado no arquivo TSV especificado
df_filtrado.to_csv(caminho_do_arquivo, sep='\t', index=False)

#FAZENDO TABELA

# Removendo duplicatas baseado na coluna correta e mantendo a primeira ocorrência
# Substitua 'Taxa' pelo nome correto da coluna de taxas em df_filtrado, se necessário
df_unico = df_filtrado.drop_duplicates(subset='Taxa')

# Criando um novo DataFrame com apenas as colunas 'Taxa', 'Contagem', e 'Presença'
# Aqui, assumimos que o valor de 'Contagem' e 'Presença' são consistentes para cada 'Taxa' única no df_filtrado
df_final = df_unico[['Taxa', 'Contagem', 'Presenca']]

# Exibindo o novo DataFrame com taxons únicos e suas respectivas contagens e presença
print(df_final)

# Definindo o caminho completo para o arquivo TSV
caminho_do_arquivo = '../tabela-final-ancombc-adultos-sem-filtro.tsv'

# Salvando o DataFrame df_filtrado no arquivo TSV especificado
df_final.to_csv(caminho_do_arquivo, sep='\t', index=False)

#---------------Essa parte irá cruzar o dataframe final de bebês vs adultos e retornar apenas os taxons consistentes

import pandas as pd

import numpy as np

df1 = pd.read_csv('../tabela-final-ancombc-bebes-sem-filtro.tsv', sep='\t')

df2 = pd.read_csv('../tabela-final-ancombc-adultos-sem-filtro.tsv', sep='\t')

# Verificar se ambos os DataFrames têm a coluna 'Presenca'
if 'Presenca' in df1.columns and 'Presenca' in df2.columns:
    # Cruzar os DataFrames com base em uma chave comum, por exemplo, 'Taxa'
    merged_df = pd.merge(df1, df2, on='Taxa', suffixes=('_df1', '_df2'))
    
    # Criar uma nova coluna 'Presence' com base nas condições
    conditions = [
        (merged_df['Presenca_df1'] == 'Only enriched') & (merged_df['Presenca_df2'] == 'Only enriched'),
        (merged_df['Presenca_df1'] == 'Only depleted') & (merged_df['Presenca_df2'] == 'Only depleted')
    ]
    choices = ['Enriched in both', 'Depleted in both']
    merged_df['Presence'] = np.select(conditions, choices, default='Not present in both')
    
    # Manter apenas as colunas desejadas
    final_df = merged_df[['Taxa', 'Contagem_df1', 'Contagem_df2', 'Presence']]
    
    # Salvar o resultado final
    final_df.to_csv('../final_result.tsv', sep='\t', index=False)
else:
    print('A coluna "Presenca" não está presente em ambos os DataFrames.')
